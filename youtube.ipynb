{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterizing YouTube Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Created by three former PayPal employees in 2005 and later acquired in 2006 by Google, YouTube has tranformed from a simple video hosting service into the world's largest entertainment platform. A community for creators everywhere, YouTube has proven able to support its creators financially as well through Google's targeted advertising program AdSense, and people are using that to quit their jobs and focus on their YouTube channels as a full-time career. In fact, in an age where anyone can pick up their phones and start a vlog, the question of what makes a YouTube channel successful is hotly debated. Today, we'll be looking at channels across several genres (lifestyle, food & travel, gaming, beauty & fashion) to get an idea of what being a successful YouTuber looks like.\n",
    "\n",
    "#### Q: What does a successful YouTube channel look like?\n",
    "\n",
    "To break this down even further, we're going to answer this question by carrying out the following steps:\n",
    "\n",
    "1. [Dataset Assembly](#1.-Dataset-Assembly)\n",
    "2. [Feature Engineering](#2.-Feature-Engineering)\n",
    "3. [Model Training](#3.-Model-Training)\n",
    "4. [Cluster Visualization](#4.-Cluster-Visualization)\n",
    "5. [Future Directions](#5.-Future-Directions)\n",
    "\n",
    "Without further ado, let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Assembly\n",
    "\n",
    "To assemble our dataset, we're going to query the YouTube Data API's search function for our genres (lifestyle, food & travel, gaming, beauty & fashion). Before we do this, however, it would be helpful to define the 3 major types of channels we expect to find.\n",
    "\n",
    "### Channel Types\n",
    "\n",
    "#### Popular\n",
    "\n",
    "A **popular** channel has popular videos (large view/video count), a large following (large subscriber count), and an active community (buzzing comment section) around it. Popular channels may not necessarily have the highest view count per video, but accumulate views through consistent content release and the community around them.\n",
    "\n",
    "#### Growing\n",
    "\n",
    "A **growing** channel is like a popular channel except it only started gaining traction recently so its community activity may not be as consistent. Many channels with growing view counts and subscriber counts exist on YouTube, but the key distinction for the growing channels we've defined is the consistency of its content release schedules since channels only become popular if they engage their audience consistently.\n",
    "\n",
    "#### Small\n",
    "\n",
    "A **small** channel is lesser known within the YouTube community. It generally has low view counts and subscriber counts relative to popular and growing channels.\n",
    "\n",
    "### Sampling Bias\n",
    "\n",
    "To understand the motivation behind asking our question, consider music as a genre on YouTube. Vevo is a music video hosting service that partners with huge records labels like the Warner Music Group, and as a part of their contracts, Vevo helps artists like Eminem and Rihanna manage their YouTube channels. As we've defined above, channels like EminemVEVO and RihannaVEVO would be considered **popular** channels, but their success is largely derived from the artists' success in the music industry and other platforms like Spotify. \n",
    "\n",
    "Because of this, we're not extremely interested in investigating what makes these YouTube channels successful because we suspect that a big part of their success is independent of the YouTube channels themselves. That's why we're not considering music as a genre when we're assembling this dataset and why we're not looking into other genres like late night talk shows because most of them present the same confounding variables that influence the success of their channels.\n",
    "\n",
    "### YouTube Data API\n",
    "\n",
    "For querying the YouTube database, we'll be using the YouTube Data API. Here is a link to its official documentation: https://developers.google.com/youtube/v3/.\n",
    "\n",
    "More specifically, we're interested in using a method called **search: list**, which will allow us to query the database with parameters like keywords, location, etc. More information about this method can be found here: https://developers.google.com/youtube/v3/docs/search/list.\n",
    "\n",
    "We start by instantiating an object that establishes an authenticated connection to the YouTube Data Api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from httplib2 import Http\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "import dateutil.parser\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_SERVICE_NAME = \"youtube\"\n",
    "API_VERSION = \"v3\"\n",
    "API_KEY = \"AIzaSyDI8cZyqHiXp1uh9zr5qPRKe4-bhhaPYUw\" # use your Google Developers Console API key\n",
    "\n",
    "def get_authenticated_service():\n",
    "    return build(API_SERVICE_NAME, API_VERSION, http=Http(), developerKey=API_KEY)\n",
    "\n",
    "client = get_authenticated_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use this object to make requests to the YouTube Data API. Below, for example, we have a code snippet that gets 25 channels about ukuleles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Ukulele Teacher\n",
      "UC1HlihY-iNtOemAlYQq3GXQ\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"part\": \"snippet\",\n",
    "    \"maxResults\": \"25\",\n",
    "    \"q\": \"ukulele\",\n",
    "    \"type\": \"channel\"\n",
    "}\n",
    "\n",
    "response = client.search().list(**params).execute()\n",
    "print(response['items'][0]['snippet']['title'])\n",
    "print(response['items'][0]['id']['channelId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the demo code above to query for up to 50 (limit) channels related to each of our topics and add them to a pandas dataframe where we store our dataset. To ensure we obtain as much data for preprocessing as possible, we're considering 3 orderings: relevance, video count, and view count. This will give us something closer to 700 channels rather than 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel ID</th>\n",
       "      <th>Channel Name</th>\n",
       "      <th>Created</th>\n",
       "      <th>Description</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>UCXFazK2sRYkuNgZ_Xs7BKUg</td>\n",
       "      <td>Batman Fitness</td>\n",
       "      <td>2016-11-06</td>\n",
       "      <td>Salut à tous je suis Batman fitness de la vérité.</td>\n",
       "      <td>Fitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>UCtE1l7hJ1helcsyxoquNDvQ</td>\n",
       "      <td>NBO FITNESS</td>\n",
       "      <td>2012-06-29</td>\n",
       "      <td>YOUTUBE FITNESS PERSONALITY, TRAINER, FAMILY M...</td>\n",
       "      <td>Fitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>UCrF6sYzdIgo64yi5GlcSsDw</td>\n",
       "      <td>Pain &amp; Gain Fitness</td>\n",
       "      <td>2011-03-28</td>\n",
       "      <td>Management: Christian Torres Video Edit: Denis...</td>\n",
       "      <td>Fitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>UCYi4JJcjDYtNGLX0N6e7g5A</td>\n",
       "      <td>Men's Health &amp; Fitness Tips</td>\n",
       "      <td>2015-07-15</td>\n",
       "      <td>Men Health &amp; Fitness and Sexual Tips , This Ch...</td>\n",
       "      <td>Fitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>UCLDw7ummSJnILbMZn2Azf2g</td>\n",
       "      <td>ON THE RADAR</td>\n",
       "      <td>2011-12-04</td>\n",
       "      <td>ON THE RADAR IS YOUR ULTIMATE RESOURCE FOR Hea...</td>\n",
       "      <td>Fitness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Channel ID                 Channel Name     Created  \\\n",
       "689  UCXFazK2sRYkuNgZ_Xs7BKUg               Batman Fitness  2016-11-06   \n",
       "690  UCtE1l7hJ1helcsyxoquNDvQ                  NBO FITNESS  2012-06-29   \n",
       "691  UCrF6sYzdIgo64yi5GlcSsDw          Pain & Gain Fitness  2011-03-28   \n",
       "692  UCYi4JJcjDYtNGLX0N6e7g5A  Men's Health & Fitness Tips  2015-07-15   \n",
       "693  UCLDw7ummSJnILbMZn2Azf2g                 ON THE RADAR  2011-12-04   \n",
       "\n",
       "                                           Description    Genre  \n",
       "689  Salut à tous je suis Batman fitness de la vérité.  Fitness  \n",
       "690  YOUTUBE FITNESS PERSONALITY, TRAINER, FAMILY M...  Fitness  \n",
       "691  Management: Christian Torres Video Edit: Denis...  Fitness  \n",
       "692  Men Health & Fitness and Sexual Tips , This Ch...  Fitness  \n",
       "693  ON THE RADAR IS YOUR ULTIMATE RESOURCE FOR Hea...  Fitness  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset(topics, n=50):\n",
    "    dataset = {\n",
    "        'Channel ID': [],\n",
    "        'Channel Name': [],\n",
    "        'Description': [],\n",
    "        'Created': [],\n",
    "        'Genre': []\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"maxResults\": n,\n",
    "        \"relevanceLanguage\": \"en\",\n",
    "        \"type\": \"channel\"\n",
    "    }\n",
    "\n",
    "    channels = set()\n",
    "    orders = ['relevance', 'viewCount', 'videoCount']\n",
    "    \n",
    "    for topic in topics:\n",
    "        params['q'] = topic\n",
    "        for order in orders:\n",
    "            params['order'] = order\n",
    "            response = client.search().list(**params).execute()\n",
    "\n",
    "            for channel in response['items']:\n",
    "                channel_id = channel['id']['channelId']\n",
    "                channel_name = channel['snippet']['title']\n",
    "                channel_description = channel['snippet']['description']\n",
    "                channel_created = channel['snippet']['publishedAt']\n",
    "                channel_created = channel_created[:channel_created.find('T')]\n",
    "\n",
    "                if channel_id not in channels:\n",
    "                    dataset['Channel ID'].append(channel_id)\n",
    "                    dataset['Channel Name'].append(channel_name)\n",
    "                    dataset['Description'].append(channel_description)\n",
    "                    dataset['Created'].append(channel_created)\n",
    "                    dataset['Genre'].append(topic)\n",
    "                    \n",
    "                    channels.add(channel_id)\n",
    "\n",
    "    return pd.DataFrame.from_dict(dataset)\n",
    "\n",
    "topics = ['Vlog', 'Food', 'Gaming', 'Beauty', 'Fashion', 'Fitness']\n",
    "init_dataset = create_dataset(topics)\n",
    "init_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extension\n",
    "\n",
    "Awesome! We have now assembled a dataset around channels in the topics that we care about, but the dataset is lacking in features. Ideally, we would have additional information about the channel so we can characterize them into the 3 types we defined earlier.\n",
    "\n",
    "Below, we will extend our dataset to include the following features about each channel:\n",
    "- view count\n",
    "- subscriber count\n",
    "- video count\n",
    "\n",
    "We'll take these simple channel statistics and engineering some features like Views/Subscriber, which will give us an idea of how much each subscriber to the channel is contributing to the views on that channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(channel, params):\n",
    "    params['id'] = channel\n",
    "    res = client.channels().list(**params).execute()\n",
    "    return res['items'][0]['statistics']\n",
    "\n",
    "def extend_features(dataset):\n",
    "    params = {'part': 'statistics'}\n",
    "    extended = {\n",
    "        'Channel ID': [],\n",
    "        'View Count': [],\n",
    "        'Subscriber Count': [],\n",
    "        'Video Count': [],\n",
    "        'Views/Subscriber': [],\n",
    "        'Views/Video': [],\n",
    "        'Subscriber/Video': []\n",
    "    }\n",
    "        \n",
    "    for channel_id in dataset['Channel ID']:\n",
    "        channel_stats = get_channel_stats(channel_id, params)\n",
    "                \n",
    "        viewCount = int(channel_stats['viewCount'])\n",
    "        subscriberCount = int(channel_stats['subscriberCount'])\n",
    "        videoCount = int(channel_stats['videoCount'])\n",
    "        \n",
    "        if videoCount == 0 or viewCount == 0 or subscriberCount == 0:\n",
    "            continue\n",
    "        \n",
    "        extended['Channel ID'].append(channel_id)\n",
    "        extended['View Count'].append(viewCount)\n",
    "        extended['Subscriber Count'].append(subscriberCount)\n",
    "        extended['Video Count'].append(videoCount)\n",
    "        extended['Views/Subscriber'].append(viewCount/subscriberCount)\n",
    "        extended['Views/Video'].append(viewCount/videoCount)\n",
    "        extended['Subscriber/Video'].append(subscriberCount/videoCount)\n",
    "    \n",
    "    extended_dataset = pd.DataFrame.from_dict(extended).set_index('Channel ID')\n",
    "    return dataset.join(extended_dataset, on=\"Channel ID\", how=\"inner\")\n",
    "    \n",
    "channel_features_dataset = extend_features(init_dataset)\n",
    "channel_features_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Features\n",
    "\n",
    "#### 1. Community Activity\n",
    "\n",
    "Earlier, we identified a channel's community as an important metric in measuring its popularity, and a channel's comment section best reflects this. For this we will query each channels last 50 videos and average across the comments for these videos.\n",
    "\n",
    "#### 2. Content Consistency\n",
    "\n",
    "Additionally, our sampling targets consistent uploaders because we assume that the uploader is active so we will use these videos to look for how far apart the dates between uploads are on average.\n",
    "\n",
    "#### 3. Growth Rate\n",
    "\n",
    "To also characterize whether a channel is growing or declining, we will use the channel's percentage change in view counts across these videos by fitting a linear regression line onto these datapoints.\n",
    "\n",
    "#### 4. Favorability\n",
    "\n",
    "Lastly, each video has a statistics on the number of likes and dislikes, which is the only venue through which users can give direct binary feedback on the content uploaders post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec_to_day = lambda x: np.round(x/86400, 1)\n",
    "\n",
    "def get_video_stats(video, params):\n",
    "    try:\n",
    "        params['id'] = video\n",
    "        res = client.videos().list(**params).execute()\n",
    "        stats = res['items'][0]['statistics']\n",
    "        stats['publishedAt'] = res['items'][0]['snippet']['publishedAt']\n",
    "        return (True, stats)\n",
    "    except:\n",
    "        return (False, None)\n",
    "    \n",
    "def get_channel_videos(channel, n=50):\n",
    "    channel_params = {\n",
    "        'part': 'contentDetails',\n",
    "        'id' : channel\n",
    "    }\n",
    "    res = client.channels().list(**channel_params).execute()\n",
    "    upload_id = res['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    playlist_params = {\n",
    "        'part': 'contentDetails',\n",
    "        'playlistId' : upload_id,\n",
    "        'maxResults' : n\n",
    "    }\n",
    "    uploads = client.playlistItems().list(**playlist_params).execute()\n",
    "    return uploads['items']\n",
    "\n",
    "def calc_channel_video_stats(dates, views, likes, dislikes, comments):\n",
    "    video_stats = dict()\n",
    "    \n",
    "    views, likes, dislikes, comments = np.array(views), np.array(likes), np.array(dislikes), np.array(comments)\n",
    "    dates = list(map(lambda x: dateutil.parser.parse(x), dates))\n",
    "        \n",
    "    # average views per upload\n",
    "    video_stats['Views/Upload'] = np.mean(views)\n",
    "    # average likes per upload\n",
    "    video_stats['Likes/Upload'] = np.mean(likes)\n",
    "    # average dislikes per upload\n",
    "    video_stats['Dislikes/Upload'] = np.mean(dislikes)\n",
    "    # average comments per upload\n",
    "    video_stats['Comments/Upload'] = np.mean(comments)\n",
    "    # like to views ratio\n",
    "    video_stats['Likes/View'] = np.mean(likes/views)*100\n",
    "    # dislikes to views ratio\n",
    "    video_stats['Dislikes/View'] = np.mean(dislikes/views)*100\n",
    "    # comments to views ratio\n",
    "    video_stats['Comments/View'] = np.mean(comments/views)*100\n",
    "    # upload frequency\n",
    "    time_diff = (dates[-1] - dates[0]).total_seconds()\n",
    "    video_stats['Days/Upload'] = sec_to_day(time_diff / len(dates))\n",
    "\n",
    "    upload_days = list(map(lambda x: sec_to_day((x - dates[0]).total_seconds()), dates))\n",
    "    m, b, r, p, err = stats.linregress(upload_days, views)\n",
    "    # growth rate (views)\n",
    "    video_stats['Growth Rate'] = (m / views[0]) * 100\n",
    "    \n",
    "    return video_stats\n",
    "    \n",
    "def get_channel_video_stats(channel):\n",
    "    published_dates = []\n",
    "    view_counts = []\n",
    "    like_counts = []\n",
    "    dislike_counts = []\n",
    "    comment_counts = []\n",
    "    \n",
    "    video_params = { 'part': 'snippet,statistics' }\n",
    "    \n",
    "    channel_videos = get_channel_videos(channel)\n",
    "    \n",
    "    if len(channel_videos) == 0:\n",
    "        return None\n",
    "        \n",
    "    for i in range(len(channel_videos)-1, -1, -1):\n",
    "        video_info = channel_videos[i]\n",
    "        video_id = video_info['contentDetails']['videoId']\n",
    "        \n",
    "        success, video_stats = get_video_stats(video_id, video_params)\n",
    "\n",
    "        # publishedAt, viewCount, likeCount, dislikeCount, commentCount\n",
    "        if not success:\n",
    "            continue\n",
    "            \n",
    "        published_dates.append(video_stats['publishedAt'])\n",
    "        \n",
    "        view_counts.append(int(video_stats['viewCount']) if 'viewCount' in video_stats else 0)\n",
    "        like_counts.append(int(video_stats['likeCount']) if 'likeCount' in video_stats else 0)\n",
    "        dislike_counts.append(int(video_stats['dislikeCount']) if 'dislikeCount' in video_stats else 0)\n",
    "        comment_counts.append(int(video_stats['commentCount']) if 'commentCount' in video_stats else 0)\n",
    "                        \n",
    "    return calc_channel_video_stats(published_dates, view_counts, like_counts, dislike_counts, comment_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels with no videos returned are filtered out (ex: UCOpNcN46UbXVtpKMrmU4Abg)\n",
    "\n",
    "def extend_video_features(data):\n",
    "    extended_video = { 'Channel ID': [] }\n",
    "        \n",
    "    for channel_id in data['Channel ID']:            \n",
    "        channel_video_stats = get_channel_video_stats(channel_id)\n",
    "        \n",
    "        if channel_video_stats == None:\n",
    "            continue\n",
    "                    \n",
    "        extended_video['Channel ID'].append(channel_id)\n",
    "        for video_stat in channel_video_stats:\n",
    "            if video_stat not in extended_video:\n",
    "                extended_video[video_stat] = []\n",
    "            extended_video[video_stat].append(channel_video_stats[video_stat])\n",
    "                                        \n",
    "    extended_dataset = pd.DataFrame.from_dict(extended_video).set_index('Channel ID')\n",
    "    \n",
    "    return data.join(extended_dataset, on=\"Channel ID\", how=\"inner\")\n",
    "\n",
    "dataset = extend_video_features(channel_features_dataset)\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "\n",
    "We now have 15 features on 700 channels on its most recent 50 videos that we'll store in a csv file \"youtube-data.csv\", and we're ready to extract the important features and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('youtube-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all our assembled dataset, we need to apply some normalization methods to ensure that our KMeans++ clustering can give us the most meaningful results possible. To do so, we will apply feature scaling to bring the variance of our clusters closer together and apply kernel PCA as well to increase the dimensionality of our data so PCA can identify non-linear principal components.\n",
    "\n",
    "#### Feature Normalization\n",
    "On of the most common ways to standardize data across a dimension is to reduce the dimension to zero mean and unit variance. Let's apply the formula below for each feature dimension and see how our data changes. \n",
    "\n",
    "<center>$X^{(m)} = \\frac{X^{(m)} - \\overline{X^{(m)}}}{\\sigma}$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel ID</th>\n",
       "      <th>Channel Name</th>\n",
       "      <th>Created</th>\n",
       "      <th>Description</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Subscriber Count</th>\n",
       "      <th>Subscriber/Video</th>\n",
       "      <th>Video Count</th>\n",
       "      <th>View Count</th>\n",
       "      <th>Views/Subscriber</th>\n",
       "      <th>Views/Video</th>\n",
       "      <th>Comments/Upload</th>\n",
       "      <th>Comments/View</th>\n",
       "      <th>Days/Upload</th>\n",
       "      <th>Dislikes/Upload</th>\n",
       "      <th>Dislikes/View</th>\n",
       "      <th>Growth Rate</th>\n",
       "      <th>Likes/Upload</th>\n",
       "      <th>Likes/View</th>\n",
       "      <th>Views/Upload</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>UCjKr2Ro-5X0BM6UptvhWvEw</td>\n",
       "      <td>The Ultimate Fashion History</td>\n",
       "      <td>10/26/12</td>\n",
       "      <td>FASHION HISTORY LIKE YOU'VE NEVER LEARNED IT B...</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>-0.431295</td>\n",
       "      <td>-0.341956</td>\n",
       "      <td>-0.448291</td>\n",
       "      <td>-0.311062</td>\n",
       "      <td>-0.629982</td>\n",
       "      <td>-0.311746</td>\n",
       "      <td>-0.272409</td>\n",
       "      <td>-0.563164</td>\n",
       "      <td>0.144017</td>\n",
       "      <td>-0.234135</td>\n",
       "      <td>-0.253131</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>-0.326700</td>\n",
       "      <td>0.398713</td>\n",
       "      <td>-0.236878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>UCS-HyxHT3A_PLXC8zlX73Yw</td>\n",
       "      <td>Fashion Television</td>\n",
       "      <td>11/10/14</td>\n",
       "      <td>Fashion Television is considered the leading a...</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>-0.435253</td>\n",
       "      <td>-0.363157</td>\n",
       "      <td>-0.143750</td>\n",
       "      <td>-0.306632</td>\n",
       "      <td>0.783178</td>\n",
       "      <td>-0.318565</td>\n",
       "      <td>-0.272409</td>\n",
       "      <td>-0.563164</td>\n",
       "      <td>-0.597007</td>\n",
       "      <td>-0.235434</td>\n",
       "      <td>-0.287314</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>-0.331377</td>\n",
       "      <td>-0.922151</td>\n",
       "      <td>-0.239402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>UCoc8tpGCY1wrp8pV7mI0scA</td>\n",
       "      <td>H&amp;M</td>\n",
       "      <td>3/7/07</td>\n",
       "      <td>Welcome to H&amp;M's official YouTube page. Explor...</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>-0.294380</td>\n",
       "      <td>-0.270066</td>\n",
       "      <td>-0.286767</td>\n",
       "      <td>-0.043644</td>\n",
       "      <td>1.182617</td>\n",
       "      <td>0.005803</td>\n",
       "      <td>-0.272409</td>\n",
       "      <td>-0.563164</td>\n",
       "      <td>0.367722</td>\n",
       "      <td>-0.065151</td>\n",
       "      <td>-0.105284</td>\n",
       "      <td>0.475871</td>\n",
       "      <td>-0.289295</td>\n",
       "      <td>-1.015866</td>\n",
       "      <td>0.880160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>UCkkVe_1wVVhT_w_BU6CKgXw</td>\n",
       "      <td>Fashion9tv</td>\n",
       "      <td>8/10/16</td>\n",
       "      <td>fashion9tv channel is reference channel for ge...</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>-0.416403</td>\n",
       "      <td>-0.362802</td>\n",
       "      <td>1.041181</td>\n",
       "      <td>-0.286283</td>\n",
       "      <td>0.387297</td>\n",
       "      <td>-0.318919</td>\n",
       "      <td>-0.272409</td>\n",
       "      <td>-0.563164</td>\n",
       "      <td>-0.708859</td>\n",
       "      <td>-0.235288</td>\n",
       "      <td>0.016218</td>\n",
       "      <td>-0.010982</td>\n",
       "      <td>-0.331367</td>\n",
       "      <td>-1.258717</td>\n",
       "      <td>-0.239284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>UCPlqUggohC1vldNi3biHkvA</td>\n",
       "      <td>Fitness Incentive</td>\n",
       "      <td>10/18/10</td>\n",
       "      <td>Watch, learn and enjoy Fitness incentive instr...</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>-0.439641</td>\n",
       "      <td>-0.364862</td>\n",
       "      <td>-0.204322</td>\n",
       "      <td>-0.313200</td>\n",
       "      <td>1.446982</td>\n",
       "      <td>-0.323333</td>\n",
       "      <td>-0.272409</td>\n",
       "      <td>-0.563164</td>\n",
       "      <td>-0.051725</td>\n",
       "      <td>-0.235467</td>\n",
       "      <td>-0.414546</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>-0.331404</td>\n",
       "      <td>-1.356894</td>\n",
       "      <td>-0.239401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Channel ID                  Channel Name   Created  \\\n",
       "604  UCjKr2Ro-5X0BM6UptvhWvEw  The Ultimate Fashion History  10/26/12   \n",
       "605  UCS-HyxHT3A_PLXC8zlX73Yw            Fashion Television  11/10/14   \n",
       "606  UCoc8tpGCY1wrp8pV7mI0scA                           H&M    3/7/07   \n",
       "607  UCkkVe_1wVVhT_w_BU6CKgXw                    Fashion9tv   8/10/16   \n",
       "608  UCPlqUggohC1vldNi3biHkvA             Fitness Incentive  10/18/10   \n",
       "\n",
       "                                           Description    Genre  \\\n",
       "604  FASHION HISTORY LIKE YOU'VE NEVER LEARNED IT B...  Fashion   \n",
       "605  Fashion Television is considered the leading a...  Fashion   \n",
       "606  Welcome to H&M's official YouTube page. Explor...  Fashion   \n",
       "607  fashion9tv channel is reference channel for ge...  Fashion   \n",
       "608  Watch, learn and enjoy Fitness incentive instr...  Fitness   \n",
       "\n",
       "     Subscriber Count  Subscriber/Video  Video Count  View Count  \\\n",
       "604         -0.431295         -0.341956    -0.448291   -0.311062   \n",
       "605         -0.435253         -0.363157    -0.143750   -0.306632   \n",
       "606         -0.294380         -0.270066    -0.286767   -0.043644   \n",
       "607         -0.416403         -0.362802     1.041181   -0.286283   \n",
       "608         -0.439641         -0.364862    -0.204322   -0.313200   \n",
       "\n",
       "     Views/Subscriber  Views/Video  Comments/Upload  Comments/View  \\\n",
       "604         -0.629982    -0.311746        -0.272409      -0.563164   \n",
       "605          0.783178    -0.318565        -0.272409      -0.563164   \n",
       "606          1.182617     0.005803        -0.272409      -0.563164   \n",
       "607          0.387297    -0.318919        -0.272409      -0.563164   \n",
       "608          1.446982    -0.323333        -0.272409      -0.563164   \n",
       "\n",
       "     Days/Upload  Dislikes/Upload  Dislikes/View  Growth Rate  Likes/Upload  \\\n",
       "604     0.144017        -0.234135      -0.253131    -0.004679     -0.326700   \n",
       "605    -0.597007        -0.235434      -0.287314     0.001706     -0.331377   \n",
       "606     0.367722        -0.065151      -0.105284     0.475871     -0.289295   \n",
       "607    -0.708859        -0.235288       0.016218    -0.010982     -0.331367   \n",
       "608    -0.051725        -0.235467      -0.414546     0.002601     -0.331404   \n",
       "\n",
       "     Likes/View  Views/Upload  \n",
       "604    0.398713     -0.236878  \n",
       "605   -0.922151     -0.239402  \n",
       "606   -1.015866      0.880160  \n",
       "607   -1.258717     -0.239284  \n",
       "608   -1.356894     -0.239401  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_data = pd.read_csv('youtube-data.csv')\n",
    "\n",
    "dataset = orig_data.drop(['Unnamed: 0'], axis=1)\n",
    "dataset = dataset.replace([np.inf, -np.inf], np.nan)\n",
    "dataset = dataset.dropna(axis=0, how=\"any\")\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "\n",
    "def standardize_features(X):\n",
    "    return (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "def normalize_data(data):\n",
    "    result = data.copy(deep=True)\n",
    "    for column in data.columns:\n",
    "        if data[column].dtype == object:\n",
    "            continue\n",
    "        result[column] = standardize_features(data[column])\n",
    "    return result\n",
    "\n",
    "norm_dataset = normalize_data(dataset)\n",
    "norm_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA\n",
    "\n",
    "Next, we will apply a Gaussian RBF (Radial Basis Function) kernel to map our data into a higher dimensional space so we can identify a potentially non-linear lower dimensional subspace for our principal components. This is done by applying the function below to each pair of feature vectors.\n",
    "\n",
    "<center>$K(x,y) = exp(-\\frac{||x-y||^2}{2\\sigma^2})$</center>\n",
    "\n",
    "Additionally, we'll also apply a median distance trick and use that as the standard deviation across our samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def calc_median_dist(X):\n",
    "    dists = []\n",
    "    n,m = X.shape\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dist = np.sqrt(np.sum((X[i,:]-X[j,:])**2))\n",
    "            dists.append(dist)\n",
    "    return np.median(dists)\n",
    "\n",
    "def drop_non_numerical(data):\n",
    "    drop_cols = []\n",
    "    for column in data.columns:\n",
    "        if data[column].dtype == object:\n",
    "            drop_cols.append(column)\n",
    "    return data.drop(drop_cols, axis=1)\n",
    "\n",
    "def kernel_PCA(data, n=6):\n",
    "    num_data = drop_non_numerical(data)\n",
    "    num_data = num_data.as_matrix()\n",
    "    \n",
    "    median_dist = calc_median_dist(num_data)\n",
    "    gamma = 1/(2*median_dist**2)\n",
    "    rbf = np.exp(-gamma * (np.sum(num_data**2, axis=-1)[:,None] + np.sum(num_data**2, axis=-1)[None,:] - 2*np.dot(num_data, num_data.T)))\n",
    "    pca = PCA(n_components=n)\n",
    "    return pca.fit_transform(rbf)\n",
    "    \n",
    "kernel_data = kernel_PCA(norm_dataset)\n",
    "print(kernel_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have transformed our dataset into several potentially non-linear principal components, we will train a clustering model to see what major clusters these channels form.\n",
    "\n",
    "We chose the KMeans++ model because we think that similar channels (popular, growing, declining) should share similar properties based on these identified metrics. We initially chose to train on $k=4$ clusters because we wanted an additional bucket for channels we cannot successfully label as one of our 3 types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From dzq homework 5\n",
    "import math\n",
    "\n",
    "def distance_matrix(X):\n",
    "    m,n = X.shape\n",
    "    M = np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            M[i,j] = np.sum((X[i]-X[j])**2)\n",
    "    return M\n",
    "\n",
    "class KMeans:\n",
    "    def init_centers(self, X, k):\n",
    "        centers = []\n",
    "        m = X.shape[0]\n",
    "        M = distance_matrix(X)\n",
    "        pos = np.arange(len(X))\n",
    "        for i in range(k):\n",
    "            if i == 0:\n",
    "                center = X[np.random.choice(pos)]\n",
    "            else:\n",
    "                # Calculate probabilities\n",
    "                probs = []\n",
    "                for x in range(m):\n",
    "                    # Calculate each vector's probability\n",
    "                    sub_probs = []\n",
    "                    for y in range(m):\n",
    "                        sub_prob = []\n",
    "                        for center in centers:\n",
    "                            dist_to_center = np.sum((X[y]-center)**2)\n",
    "                            sub_prob.append(dist_to_center)\n",
    "                        val = np.min(sub_prob)\n",
    "                        sub_probs.append(val)\n",
    "                    \n",
    "                    num = sub_probs[x]\n",
    "                    denom = np.sum(sub_probs)\n",
    "                    \n",
    "                    prob = num/denom\n",
    "                    probs.append(prob)\n",
    "                    \n",
    "                center = X[np.random.choice(pos, p=probs)]\n",
    "                \n",
    "            centers.append(center)\n",
    "            \n",
    "        centers = np.array(centers)\n",
    "        return centers\n",
    "        \n",
    "    def assign_clusters(self, X, centers):\n",
    "        m,k = X.shape[0], len(centers) \n",
    "        clusters = np.zeros((m,k))\n",
    "        for i in range(m):\n",
    "            probs = []\n",
    "            for j in range(k):\n",
    "                dist = np.sum((X[i]-centers[j])**2)\n",
    "                probs.append(dist)\n",
    "            y = np.argmin(probs)\n",
    "            clusters[i,y] = 1\n",
    "        return clusters\n",
    "    \n",
    "    def compute_means(self, X, y):\n",
    "        m, n, k = X.shape[0], X.shape[1], y.shape[1]\n",
    "        centers = np.zeros((k,n))\n",
    "        \n",
    "        for j in range(k):\n",
    "            cluster = np.zeros(n)\n",
    "            num_cluster = 0\n",
    "            for i in range(m):\n",
    "                if y[i,j] == 1:\n",
    "                    num_cluster += 1\n",
    "                    cluster = cluster + X[i]\n",
    "            center = cluster / num_cluster\n",
    "            centers[j,:] = center\n",
    "            \n",
    "        return centers    \n",
    "    \n",
    "    def train(self, X, centers, niters=500):\n",
    "        for i in range(niters):\n",
    "            clusters = self.assign_clusters(X, centers)\n",
    "            centers = self.compute_means(X, clusters)\n",
    "                    \n",
    "        return (clusters, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running KMeans++, we can see the size of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:  353\n",
      "Cluster 1:  68\n",
      "Cluster 2:  89\n",
      "Cluster 3:  99\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "KM = KMeans()\n",
    "mu = KM.init_centers(kernel_data, k)\n",
    "(labels, centers) = KM.train(kernel_data, mu)\n",
    "\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d: \" % i, len(np.where(labels[:,i]==1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what channels are in our largest cluster, cluster 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38                                   TheReportOfTheWeek\n",
       "43                                          Family Fizz\n",
       "51                                      julien solomita\n",
       "54                                         Casey Holmes\n",
       "56                                     eleventhgorgeous\n",
       "58                                        VasseurBeauty\n",
       "59                                            8-BitRyan\n",
       "60                                    Strictly Dumpling\n",
       "61                                      FastGoodCuisine\n",
       "62                                           Mark Wiens\n",
       "65                                         Neebs Gaming\n",
       "66                                      RawBeautyKristi\n",
       "67                                      HellthyJunkFood\n",
       "68                                        Joey Graceffa\n",
       "69                             Agnieszka Grzelak Beauty\n",
       "74                                 The Life Of Us Vlogs\n",
       "75                                     Christian Guzman\n",
       "77                                          Bon Appétit\n",
       "80                                                Vogue\n",
       "81                                  Familia Carameluchi\n",
       "83                           Best Ever Food Review Show\n",
       "84                                       Sascha Fitness\n",
       "86                                      SuperwomanVlogs\n",
       "88                                          Sejal Kumar\n",
       "91                                      Get Good Gaming\n",
       "93                                 PontiacMadeDDG VLOGS\n",
       "94                     Unit Lost - Great British Gaming\n",
       "95                                 Legends of Gaming NL\n",
       "97                                            UIC Vlogs\n",
       "100                       ERNEST DIFT - THE FITNESS BOY\n",
       "                             ...                       \n",
       "554                                    FITNESS Magazine\n",
       "557                                         Shared Food\n",
       "558                                Empire Beauty School\n",
       "559                                      Fashion Trends\n",
       "564                                   Fashion Trends PK\n",
       "565                                      Fashion Trends\n",
       "566                                 Food Network Canada\n",
       "567    UK College of Agriculture, Food, and Environment\n",
       "570                              NailsRUs Beauty Supply\n",
       "571                                 Male Fashion Trends\n",
       "572                                            Food Tak\n",
       "573                                    EenaduIndia Food\n",
       "576                                    Fashion & Beauty\n",
       "579                                        Time Fitness\n",
       "580                                    The Food Channel\n",
       "583                             British Fashion Council\n",
       "585                                 U.S. Forces Fitness\n",
       "587                         Style and fashion by bassem\n",
       "588                              Moda 2017 Fashion 2018\n",
       "589                                  Lakmé Fashion Week\n",
       "590                                Sahana Beauty Corner\n",
       "591                                           HW Beauty\n",
       "592                                     Beauty at Tesco\n",
       "593                              Latest Fashion Designs\n",
       "595                                         Fashion One\n",
       "597                                   Fly Dance Fitness\n",
       "598                             Premier Fitness Systems\n",
       "601                                       ID Fashion TV\n",
       "604                        The Ultimate Fashion History\n",
       "605                                  Fashion Television\n",
       "Name: Channel Name, Length: 353, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Channel Name\"][labels[:,0] == 1] # Cluster 0 (364 channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters\n",
    "\n",
    "In this section, we will consider $k$ that will generate different cluster labels for each channel. Then, we will calculate an error using cross entropy on a curated test dataset and minimize this our objective funciton to obtain the optimal hyperparameter.\n",
    "\n",
    "#### Test Data\n",
    "\n",
    "We manually assembled this dataset by searching for popular YouTube channels online, and filtering for channels that are not already in our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extend_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-68d264ac5e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mextend_video_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_test_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_channel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-68d264ac5e86>\u001b[0m in \u001b[0;36mcreate_test_dataset\u001b[0;34m(channel_ids)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0minit_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mextend_video_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extend_features' is not defined"
     ]
    }
   ],
   "source": [
    "test_channel_ids = {\n",
    "    \"Vlog\": [\n",
    "        \"UC4-CH0epzZpD_ARhxCx6LaQ\",\n",
    "        \"UC_gV70G_Y51LTa3qhu8KiEA\",\n",
    "        \"UCcgVECVN4OKV6DH1jLkqmcA\",\n",
    "        \"UCPOw2O3_uZ1doro9iR4x6vw\",\n",
    "        \"UC0otZdGYsA9KqVKAcn2peQA\"\n",
    "    ], \"Food\": [\n",
    "        \"UCJQL1Fai-9GlVunsbP4x8Pg\",\n",
    "        \"UCRIZtPl9nb9RiXc9btSTQNw\",\n",
    "        \"UCNbngWUqL2eqRw12yAwcICg\",\n",
    "        \"UC6S5a3MQtr_PSWZxysXkOCg\",\n",
    "        \"UCffs63OaN2nh-6StR6hzfiQ\"\n",
    "    ], \"Gaming\": [\n",
    "        \"UCAW-NpUFkMyCNrvRSSGIvDQ\",\n",
    "        \"UC1uvf8YdxSVzthF45kotmJQ\",\n",
    "        \"UCbTVTephX30ZhQF5zwFppBg\",\n",
    "        \"UCS5Oz6CHmeoF7vSad0qqXfw\",\n",
    "        \"UCpGdL9Sn3Q5YWUH2DVUW1Ug\"\n",
    "    ], \"Fashion\": [\n",
    "        \"UC5zSySQab9SA6Wz569WDgqw\",\n",
    "        \"UCgWfS_47YPVbKx5EK4FLm4A\",\n",
    "        \"UCo5zIpjl2OQkYatd8R0bDaw\",\n",
    "        \"UC-BaXc1TU9i0XSguq9mZwdg\",\n",
    "        \"UC48DOiEvCDu3sThBijwkQ1A\"\n",
    "    ], \"Fitness\": [\n",
    "        \"UCXIJ2-RSIGn53HA-x9RDevA\",\n",
    "        \"UCuA6Ht35K326kBTPTXaWj3g\",\n",
    "        \"UCgBc9iNvvjWDInV6fBeTGXQ\",\n",
    "        \"UCuY1W4AwhhgkB6rsJBtltUA\",\n",
    "        \"UCMTXToEZ6VT5k9GOCFNYjWA\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def create_test_dataset(channel_ids):\n",
    "    test_dataset = { \"Channel ID\": [] }\n",
    "    for topic in channel_ids:\n",
    "        for channel_id in test_channel_ids[topic]:\n",
    "            test_dataset[\"Channel ID\"].append(channel_id)\n",
    "    \n",
    "    test_data = pd.DataFrame.from_dict(test_dataset)\n",
    "    init_test = extend_features(test_data)\n",
    "    return extend_video_features(init_test)\n",
    "            \n",
    "test_data = create_test_dataset(test_channel_ids)\n",
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's save this into a spreadsheet for easy loading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data.to_csv('youtube-test-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we apply the same cleaning and feature space transformations onto our test data we can calculate distances to the training data cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 6)\n"
     ]
    }
   ],
   "source": [
    "orig_test = pd.read_csv('youtube-test-data.csv')\n",
    "\n",
    "test_data = orig_test.drop(['Unnamed: 0'], axis=1)\n",
    "test_data = orig_test.replace([np.inf, -np.inf], np.nan)\n",
    "test_data = test_data.dropna(axis=0, how=\"any\")\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "test_norm = normalize_data(test_data)\n",
    "test_kernel = kernel_PCA(test_norm)\n",
    "print(test_kernel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss\n",
    "\n",
    "Since these channels are very similar and commonly recognized as popular channels on YouTube, we will asumme that they should be classified into similar clusters under our model. Thus, we can use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def cross_entropy(cluster_labels):\n",
    "    freqs = Counter()\n",
    "    for label in cluster_labels:\n",
    "        freqs[label] += 1\n",
    "        \n",
    "    n = len(cluster_labels)\n",
    "    entropy = 0\n",
    "    for label in freqs:\n",
    "        label_prob = freqs[label]/n\n",
    "        entropy += label_prob*np.log(label_prob)\n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a model testing function that returns the total entropy for a given model and another wrapper function that minimize cross entropy loss across models trained on different hyperparameter $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3  error=0.6897\n",
      "k=4  error=0.7924\n",
      "k=5  error=0.1732\n",
      "k=6  error=0.6160\n",
      "k=7  error=0.6520\n"
     ]
    }
   ],
   "source": [
    "def test_model(KM, centers, test_data):\n",
    "    predicted_clusters = KM.assign_clusters(test_data, centers)    \n",
    "    predicted_labels = list(map(lambda x: np.argmax(x), predicted_clusters))\n",
    "    return cross_entropy(predicted_labels)\n",
    "\n",
    "def optimize(train_data, test_data):\n",
    "    min_error = None\n",
    "    opt_k = None\n",
    "    opt_model = None\n",
    "    opt_centers = None\n",
    "    opt_labels = None\n",
    "    \n",
    "    for k in range(3,8): # 3,4,5,6,7\n",
    "        kmeans = KMeans()\n",
    "        init_centers = kmeans.init_centers(train_data, k)\n",
    "        labels, centers = kmeans.train(train_data, init_centers)\n",
    "        \n",
    "        error = test_model(kmeans, centers, test_data)\n",
    "        \n",
    "        print(\"k=%d  error=%0.4f\" % (k,error))\n",
    "        \n",
    "        if min_error == None or error < min_error:\n",
    "            min_error = error\n",
    "            opt_k = k\n",
    "            opt_model = kmeans\n",
    "            opt_centers = centers\n",
    "            opt_labels = labels\n",
    "            \n",
    "    return (opt_k, opt_model, opt_centers, opt_labels)\n",
    "        \n",
    "k, KM, opt_centers, opt_labels = optimize(kernel_data, test_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this information about our optimal hyperparameter, we can train a new KMeans++ model with $k=5$ and use that to visualize the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster Visualization\n",
    "\n",
    "We use Plotly in our cluster visualization because it supports powerful interactive graphs, and we are visualizing the assigned labels on our training data in the first 3 principal components. The plot generated can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~pdsfinal/0 or inside your plot.ly account where it is named 'cluster-3d-scatter'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~pdsfinal/0.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.tools.set_credentials_file(username=\"pdsfinal\", api_key=\"uWfUSo3kfMnXM6gTxmUO\")\n",
    "\n",
    "def visualize_clusters_3D(data, labels, k):\n",
    "    transformed = data[:, 0:3]\n",
    "    \n",
    "    plot_data = []\n",
    "    for i in range(k):\n",
    "        trace = go.Scatter3d(\n",
    "            x=transformed[np.where(labels[:,i] == 1)[0]][:,0],\n",
    "            y=transformed[np.where(labels[:,i] == 1)[0]][:,1],\n",
    "            z=transformed[np.where(labels[:,i] == 1)[0]][:,2],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=12,\n",
    "                line=dict(\n",
    "                    color='rgba(217,217,217,0.14)',\n",
    "                    width=0.5\n",
    "                ),\n",
    "                opacity=0.8\n",
    "            )\n",
    "        )\n",
    "        plot_data.append(trace)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=0,\n",
    "            b=0,\n",
    "            t=0\n",
    "        )\n",
    "    )\n",
    "    return go.Figure(data=plot_data, layout=layout)\n",
    "    \n",
    "fig = visualize_clusters_3D(kernel_data, opt_labels, k)\n",
    "py.iplot(fig, filename='cluster-3d-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, now that we have labeled data, we can visualize the clusters with respect to our original features. We will explore clusters with regards to some of those metrics below.\n",
    "\n",
    "First, we'll get the dataframe indices for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_idxs(labels, k):\n",
    "    cluster_idxs = dict()\n",
    "    for i in range(k):\n",
    "        cluster_idxs[i] = np.where(labels[:,i] == 1)[0]\n",
    "    return cluster_idxs\n",
    "\n",
    "cluster_idxs = get_cluster_idxs(opt_labels, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PCA transforms our data by projecting each feature to the principal components, the $x,y,z$ axes in the plot above don't necessarily correspond to our original features in the way we would expect it. However, we can use the clusters that our KMeans++ model has identified to visualize features we identified earlier.\n",
    "\n",
    "Here's an example of that with subscriber count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~pdsfinal/16.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_cluster_stacked_histogram(data, feature, clusters, labels, k):      \n",
    "    plot_data = []\n",
    "    for i in range(k):\n",
    "        trace = go.Histogram(\n",
    "            x = dataset[feature][clusters[i]],\n",
    "            name = \"Cluster %d\" % i\n",
    "        )\n",
    "        plot_data.append(trace)\n",
    "    \n",
    "    layout = go.Layout(\n",
    "        xaxis=dict(title=feature),\n",
    "        yaxis=dict(title=\"Count\"),\n",
    "        barmode=\"stack\")\n",
    "    \n",
    "    return go.Figure(data=plot_data, layout=layout)\n",
    "    \n",
    "fig = visualize_cluster_stacked_histogram(dataset, \"Subscriber Count\", cluster_idxs, opt_labels, k)\n",
    "py.iplot(fig, filename='cluster-subscriber-count-stacked-histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another one with upload frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~pdsfinal/18.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = visualize_cluster_stacked_histogram(dataset, \"Days/Upload\", cluster_idxs, opt_labels, k)\n",
    "py.iplot(fig, filename='cluster-days-upload-stacked-histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certain insights we've gained from visualizing the data. In the upload frequency plot above, we can see that channels from cluster 1 and 4 tend to be really frequent uploaders (videos within a day).\n",
    "\n",
    "### Cluster Types\n",
    "\n",
    "**Popular**: Cluster 3 (red) and Cluster 0 (blue) contain our most popular channels. These channels have lots of subscribers and their channels have consistently high engagement. Examples include: CaseyNeistat, Gigi Gorgeous, RomanAtwoodVlogs, and Epic Meal Time.\n",
    "\n",
    "**Growing**: Cluster 4 (purple) and Cluster 1 (orange) contain our channels with average popularity. These channels have fewer subsrcibers than the two above, but still have good audience engagement. Examples include: AlishaMarieVlogs, iJustine, Vlogs by DK4L, and Bon Appetit.\n",
    "\n",
    "**Small Channels**: Cluster 2 (green) contains our smallest channels. These channels have low engagement from their audience. Examples include: LevelCapGaming, Beauty Tricks, and Jamie Oliver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future Directions\n",
    "\n",
    "1. Analyzing genres separately\n",
    "2. Webscraping for smaller channels\n",
    "3. Create time series database\n",
    "4. Exploring other unsupervised clustering algorithms\n",
    "5. Training sentiment analysis model\n",
    "\n",
    "In this project we did not consider the differences achieving popularity across different genres. In the future we look to analyze each genre separately to reduce the effects of any confounding variables across genres.\n",
    "\n",
    "When we queried the YouTube Data API, it was inherently biased towards larger channels because they sorted in descending order on the metrics we specified in our parameters. Therefore, it might be helpful to webscrape for smaller channels for more variety in channels.\n",
    "\n",
    "The Youtube Data API does not record historical data, but we could assemble our own, which could useful to characterize growth, for example, of a channel. This is most likely what SocialBlade, a YouTube analytics platform, utilizes.\n",
    "\n",
    "Although KMeans++ is very easy to implement, the accuracy of its clusters relies on spherically distributed clusters, similar variance across clusters, etc. Thus, it could be beneficial to examine the performance of other clustering algorithms like GMM and hierarchical clustering to see what yields the lowest error values.\n",
    "\n",
    "Lastly, we were initially interested in training a sentiment analysis model to analyze video captions. We ultimately decided against it because we found that captions across even one channel's videos were highly variable. Thus, we avoided it for the purposes of this project but that could be something else to look into when expanding on our feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count: 1954"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
